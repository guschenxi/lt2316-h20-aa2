{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# aa1 stuff\n",
    "from aa1 import DataLoader\n",
    "from aa1 import extract_features\n",
    "from aa1 import check_output\n",
    "\n",
    "from aa2 import Trainer\n",
    "from aa2 import parallel_coordinates\n",
    "# <-- IMPORT YOUR MODEL CLASS HERE\n",
    "from aa2.model import MyClassifier1A\n",
    "from aa2.model import MyClassifier1B\n",
    "\n",
    "from aa2.model import MyClassifier2A\n",
    "from aa2.model import MyClassifier2B\n",
    "\n",
    "from aa2.model import MyClassifier3A\n",
    "from aa2.model import MyClassifier3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a GPU device here\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Tensor Size: torch.Size([5051, 87, 5])\n",
      "VAL Tensor Size: torch.Size([1781, 87, 5])\n",
      "TEST Tensor Size: torch.Size([665, 87, 5])\n"
     ]
    }
   ],
   "source": [
    "# assigment 1 stuff\n",
    "dataset = DataLoader(data_dir=\"DDICorpus\", device=device)\n",
    "train_y, val_y, test_y =dataset.get_y()\n",
    "train_X, val_X, test_X =extract_features(\n",
    "                                                        data=dataset.data_df,\n",
    "                                                        max_sample_length=dataset.max_sample_length,\n",
    "                                                        device=device,\n",
    "                                                        id2word=dataset.id2word,\n",
    "                                                        #vocab=dataset.vocab\n",
    "                                                        #Add any addtional arguments here\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up trainer\n",
    "model_dump = \"tmp/aa2_models/\" #you are allowed to change the dump_folder\n",
    "trainer = Trainer(dump_folder=model_dump) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a set of hyperparamaters\n",
    "# test at least 5 different sets of hyperparamaters \n",
    "set_hyperparamaters = [\n",
    "                        # Example:\n",
    "                        {\n",
    "                          \"learning_rate\": 0.005,\n",
    "                          \"number_layers\": 2,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 10, \n",
    "                          'dropout': 0.1,\n",
    "                          'hidden_size' : 32,\n",
    "                          \"epochs\": 10,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model1\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.0025,\n",
    "                          \"number_layers\": 3,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 10, \n",
    "                          'dropout': 0.2,\n",
    "                          'hidden_size' : 64,\n",
    "                          \"epochs\": 10,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model2\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.001,\n",
    "                          \"number_layers\": 2,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 50, \n",
    "                          'dropout': 0.75,\n",
    "                          'hidden_size' : 64,\n",
    "                          \"epochs\": 10,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model3\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.001,\n",
    "                          \"number_layers\": 5,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 100, \n",
    "                          'dropout': 0.5,\n",
    "                          'hidden_size' : 8,\n",
    "                          \"epochs\": 12,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model4\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.03,\n",
    "                          \"number_layers\": 8,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 50, \n",
    "                          'dropout': 0.5,\n",
    "                          'hidden_size' : 10,\n",
    "                          \"epochs\": 15,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model5\"\n",
    "                        }, \n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyClassifier2B(\n",
      "  (lin1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (lstm): LSTM(32, 32, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (linear): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "Training... \n",
      "\n",
      "Total loss in epoch 1 is 106.37700653076172.\n",
      "Total loss in epoch 2 is 95.35659790039062.\n",
      "Total loss in epoch 3 is 94.77227783203125.\n",
      "Total loss in epoch 4 is 94.4248275756836.\n",
      "Total loss in epoch 5 is 94.07653045654297.\n",
      "Total loss in epoch 6 is 94.02081298828125.\n",
      "Total loss in epoch 7 is 94.15403747558594.\n",
      "Total loss in epoch 8 is 93.72659301757812.\n",
      "Total loss in epoch 9 is 93.81851196289062.\n",
      "Total loss in epoch 10 is 93.69599151611328.\n",
      "Training Finished\n",
      "\n",
      "Evaluating... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib64/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model1 Accuracy: 0.9457620992984698 Precision: 0.9127293590473289 Recall: 0.9457620992984698 F1_score: 0.963201922850508\n",
      "MyClassifier2B(\n",
      "  (lin1): Linear(in_features=5, out_features=64, bias=True)\n",
      "  (lstm): LSTM(64, 64, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "Training... \n",
      "\n",
      "Total loss in epoch 1 is 106.27808380126953.\n",
      "Total loss in epoch 2 is 95.30253601074219.\n",
      "Total loss in epoch 3 is 94.63397979736328.\n",
      "Total loss in epoch 4 is 93.79593658447266.\n",
      "Total loss in epoch 5 is 93.67635345458984.\n",
      "Total loss in epoch 6 is 93.94158935546875.\n",
      "Total loss in epoch 7 is 93.91531372070312.\n",
      "Total loss in epoch 8 is 93.49437713623047.\n",
      "Total loss in epoch 9 is 93.74508666992188.\n",
      "Total loss in epoch 10 is 93.30187225341797.\n",
      "Training Finished\n",
      "\n",
      "Evaluating... \n",
      "\n",
      "Model: model2 Accuracy: 0.9458395451347881 Precision: 0.9218065104543289 Recall: 0.9458395451347881 F1_score: 0.962877429914052\n",
      "MyClassifier2B(\n",
      "  (lin1): Linear(in_features=5, out_features=64, bias=True)\n",
      "  (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.75, bidirectional=True)\n",
      "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "Training... \n",
      "\n",
      "Total loss in epoch 1 is 35.97118377685547.\n",
      "Total loss in epoch 2 is 19.798147201538086.\n",
      "Total loss in epoch 3 is 19.330785751342773.\n",
      "Total loss in epoch 4 is 19.267499923706055.\n",
      "Total loss in epoch 5 is 18.983997344970703.\n",
      "Total loss in epoch 6 is 18.901479721069336.\n",
      "Total loss in epoch 7 is 19.084148406982422.\n",
      "Total loss in epoch 8 is 18.86089515686035.\n",
      "Total loss in epoch 9 is 18.7004451751709.\n",
      "Total loss in epoch 10 is 18.76545524597168.\n",
      "Training Finished\n",
      "\n",
      "Evaluating... \n",
      "\n",
      "Model: model3 Accuracy: 0.9457427378393902 Precision: 0.9203574821836709 Recall: 0.9457427378393902 F1_score: 0.9631824611033295\n",
      "MyClassifier2B(\n",
      "  (lin1): Linear(in_features=5, out_features=8, bias=True)\n",
      "  (lstm): LSTM(8, 8, num_layers=5, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (linear): Linear(in_features=16, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "Training... \n",
      "\n",
      "Total loss in epoch 1 is 81.48687744140625.\n",
      "Total loss in epoch 2 is 42.24617004394531.\n",
      "Total loss in epoch 3 is 26.85845947265625.\n",
      "Total loss in epoch 4 is 18.3100643157959.\n",
      "Total loss in epoch 5 is 13.5412015914917.\n",
      "Total loss in epoch 6 is 12.093282699584961.\n",
      "Total loss in epoch 7 is 11.533207893371582.\n",
      "Total loss in epoch 8 is 11.183713912963867.\n",
      "Total loss in epoch 9 is 10.9673433303833.\n",
      "Total loss in epoch 10 is 10.807513236999512.\n",
      "Total loss in epoch 11 is 10.607858657836914.\n",
      "Total loss in epoch 12 is 10.506773948669434.\n",
      "Training Finished\n",
      "\n",
      "Evaluating... \n",
      "\n",
      "Model: model4 Accuracy: 0.9437291460951164 Precision: 0.9094587746750077 Recall: 0.9437291460951164 F1_score: 0.9614939168034238\n",
      "MyClassifier2B(\n",
      "  (lin1): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (lstm): LSTM(10, 10, num_layers=8, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (linear): Linear(in_features=20, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "Training... \n",
      "\n",
      "Total loss in epoch 1 is 37.13264083862305.\n",
      "Total loss in epoch 2 is 20.390871047973633.\n",
      "Total loss in epoch 3 is 19.804676055908203.\n",
      "Total loss in epoch 4 is 19.90097427368164.\n",
      "Total loss in epoch 5 is 19.330350875854492.\n",
      "Total loss in epoch 6 is 19.154296875.\n",
      "Total loss in epoch 7 is 18.986770629882812.\n",
      "Total loss in epoch 8 is 19.01011848449707.\n",
      "Total loss in epoch 9 is 18.97548484802246.\n",
      "Total loss in epoch 10 is 18.944236755371094.\n",
      "Total loss in epoch 11 is 19.131919860839844.\n",
      "Total loss in epoch 12 is 19.11480140686035.\n",
      "Total loss in epoch 13 is 18.86425018310547.\n",
      "Total loss in epoch 14 is 18.87217903137207.\n",
      "Total loss in epoch 15 is 18.78675651550293.\n",
      "Training Finished\n",
      "\n",
      "Evaluating... \n",
      "\n",
      "Model: model5 Accuracy: 0.9456717458227653 Precision: 0.911846177225051 Recall: 0.9456717458227653 F1_score: 0.9633283261243107\n"
     ]
    }
   ],
   "source": [
    "## hyperparamater tuning\n",
    "# train you model with your set of hyperparamaters\n",
    "for hp in set_hyperparamaters:\n",
    "    trainer.train(train_X, train_y, val_X, val_y, MyClassifier2B, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models found: ['tmp/aa2_models/model1.pt', 'tmp/aa2_models/model2.pt', 'tmp/aa2_models/model3.pt', 'tmp/aa2_models/model4.pt', 'tmp/aa2_models/model5.pt']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "comparison not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-41e43b6f11da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a parallel coordination plot over hyperparamaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# add path to models and change metric to what ever metric you have chosen to use/want to use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparallel_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/lt2316-h20-aa2/aa2/visuals.py\u001b[0m in \u001b[0;36mparallel_coordinates\u001b[0;34m(save_dir, metric)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0madd_text_ticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mmax_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mmin_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2503\u001b[0m     \"\"\"\n\u001b[1;32m   2504\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[0;32m-> 2505\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: comparison not implemented"
     ]
    }
   ],
   "source": [
    "# create a parallel coordination plot over hyperparamaters\n",
    "# add path to models and change metric to what ever metric you have chosen to use/want to use\n",
    "parallel_coordinates(model_dump, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the Parallel Coordination plot define 3 new hyperparamaters set thay you \n",
    "# think are worth testing\n",
    "set_hyperparamaters_2 = [\n",
    "                        # Example:\n",
    "                        # {\n",
    "                        #   \"learning_rate\": 0.1,\n",
    "                        #   \"number_layers\": 3,\n",
    "                        #   \"optimizer\": \"adam\"\n",
    "                        # }\n",
    "                        {\n",
    "                          \"learning_rate\": 0.005,\n",
    "                          \"number_layers\": 2,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 10, \n",
    "                          'dropout': 0.1,\n",
    "                          'hidden_size' : 32,\n",
    "                          \"epochs\": 10,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model6\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.0025,\n",
    "                          \"number_layers\": 1,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 10, \n",
    "                          'dropout': 0.1,\n",
    "                          'hidden_size' : 32,\n",
    "                          \"epochs\": 10,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model7\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.001,\n",
    "                          \"number_layers\": 2,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 20, \n",
    "                          'dropout': 0.1,\n",
    "                          'hidden_size' : 10,\n",
    "                          \"epochs\": 10,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model8\"\n",
    "                        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyClassifier2B(\n",
      "  (lin1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (lstm): LSTM(32, 32, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (linear): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "Training... \n",
      "\n",
      "Total loss in epoch 1 is 107.16030883789062.\n",
      "Total loss in epoch 2 is 95.7258529663086.\n",
      "Total loss in epoch 3 is 94.95931243896484.\n",
      "Total loss in epoch 4 is 94.93499755859375.\n",
      "Total loss in epoch 5 is 94.23291015625.\n",
      "Total loss in epoch 6 is 94.37997436523438.\n",
      "Total loss in epoch 7 is 94.28633880615234.\n",
      "Total loss in epoch 8 is 93.8508529663086.\n",
      "Total loss in epoch 9 is 93.97514343261719.\n",
      "Total loss in epoch 10 is 94.30351257324219.\n",
      "Training Finished\n",
      "\n",
      "Evaluating... \n",
      "\n",
      "Model: model6 Accuracy: 0.9444519739007532 Precision: 0.9158600342238703 Recall: 0.9444519739007532 F1_score: 0.9608218600560032\n",
      "MyClassifier2B(\n",
      "  (lin1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (lstm): LSTM(32, 32, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (linear): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "Training... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 1 is 120.99163055419922.\n",
      "Total loss in epoch 2 is 99.8780746459961.\n",
      "Total loss in epoch 3 is 96.770263671875.\n",
      "Total loss in epoch 4 is 95.45188903808594.\n",
      "Total loss in epoch 5 is 95.24820709228516.\n",
      "Total loss in epoch 6 is 94.98609924316406.\n",
      "Total loss in epoch 7 is 94.79137420654297.\n",
      "Total loss in epoch 8 is 96.72239685058594.\n",
      "Total loss in epoch 9 is 94.59394836425781.\n",
      "Total loss in epoch 10 is 94.26960754394531.\n",
      "Training Finished\n",
      "\n",
      "Evaluating... \n",
      "\n",
      "Model: model7 Accuracy: 0.9449940947549808 Precision: 0.9107146565490902 Recall: 0.9449940947549808 F1_score: 0.9627750935213161\n",
      "MyClassifier2B(\n",
      "  (lin1): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (lstm): LSTM(10, 10, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (linear): Linear(in_features=20, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "Training... \n",
      "\n",
      "Total loss in epoch 1 is 128.4783935546875.\n",
      "Total loss in epoch 2 is 52.89408493041992.\n",
      "Total loss in epoch 3 is 49.415985107421875.\n",
      "Total loss in epoch 4 is 48.4084587097168.\n",
      "Total loss in epoch 5 is 47.868648529052734.\n",
      "Total loss in epoch 6 is 47.51630401611328.\n",
      "Total loss in epoch 7 is 47.239871978759766.\n",
      "Total loss in epoch 8 is 47.04960632324219.\n",
      "Total loss in epoch 9 is 46.908206939697266.\n",
      "Total loss in epoch 10 is 46.8173713684082.\n",
      "Training Finished\n",
      "\n",
      "Evaluating... \n",
      "\n",
      "Model: model8 Accuracy: 0.9455491232485946 Precision: 0.9121527610241104 Recall: 0.9455491232485946 F1_score: 0.9630833352907279\n"
     ]
    }
   ],
   "source": [
    "# train models for the new hyperparamaters\n",
    "for hp in set_hyperparamaters_2:\n",
    "    trainer.train(train_X, train_y, val_X, val_y, MyClassifier2B, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parallel coordination plot over hyperparamaters again\n",
    "parallel_coordinates(model_dump, metric=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model1 Accuracy: 0.9650851266096275 Precision: 0.946543022025391 Recall: 0.9650851266096275 F1_score: 0.9746243047001547\n"
     ]
    }
   ],
   "source": [
    "#choose the best model base on the last parallel coordination plot\n",
    "best_model_path = \"tmp/aa2_models/model1.pt\"\n",
    "scores = trainer.test(test_X, test_y, MyClassifier2B, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
