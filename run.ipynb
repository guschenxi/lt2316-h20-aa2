{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/guschenxi@GU.GU.SE/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# aa1 stuff\n",
    "from aa1 import DataLoader\n",
    "from aa1 import extract_features\n",
    "from aa1 import check_output\n",
    "\n",
    "from aa2 import Trainer\n",
    "from aa2 import parallel_coordinates\n",
    "# <-- IMPORT YOUR MODEL CLASS HERE\n",
    "from aa2.model import MyClassifier1A\n",
    "from aa2.model import MyClassifier1B\n",
    "\n",
    "from aa2.model import MyClassifier2A\n",
    "from aa2.model import MyClassifier2B\n",
    "\n",
    "from aa2.model import MyClassifier3A\n",
    "from aa2.model import MyClassifier3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a GPU device here\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframes are ready\n",
      "split df\n",
      "tensor([], device='cuda:0', size=(0, 102), dtype=torch.int64)\n",
      "tensor([], device='cuda:0', size=(0, 102, 7), dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# assigment 1 stuff\n",
    "dataset = DataLoader(data_dir=\"DDICorpus\", device=device)\n",
    "train_y, val_y, test_y =dataset.get_y()\n",
    "train_X, val_X, test_X =extract_features(\n",
    "                                                        data=dataset.data_df,\n",
    "                                                        max_sample_length=dataset.max_sample_length,\n",
    "                                                        #device=device,\n",
    "                                                        id2w=dataset.id2word\n",
    "                                                        #Add any addtional arguments here\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up trainer\n",
    "model_dump = \"/tmp/aa2_models/\" #you are allowed to change the dump_folder\n",
    "trainer = Trainer(dump_folder=model_dump) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a set of hyperparamaters\n",
    "# test at least 5 different sets of hyperparamaters \n",
    "set_hyperparamaters = [\n",
    "                        # Example:\n",
    "                        {\n",
    "                          \"learning_rate\": 0.01,\n",
    "                          \"number_layers\": 3,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 25, \n",
    "                          'dropout': 0.2,\n",
    "                          'hidden_size' : 10,\n",
    "                          \"epochs\": 15,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model1\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.01,\n",
    "                          \"number_layers\": 10,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 100, \n",
    "                          'dropout': 0.2,\n",
    "                          'hidden_size' : 15,\n",
    "                          \"epochs\": 20,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model2\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.1,\n",
    "                          \"number_layers\": 15,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 50, \n",
    "                          'dropout': 0.2,\n",
    "                          'hidden_size' : 3,\n",
    "                          \"epochs\": 30,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model3\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.001,\n",
    "                          \"number_layers\": 5,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 100, \n",
    "                          'dropout': 0.5,\n",
    "                          'hidden_size' : 8,\n",
    "                          \"epochs\": 40,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model4\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.3,\n",
    "                          \"number_layers\": 30,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 50, \n",
    "                          'dropout': 0.5,\n",
    "                          'hidden_size' : 10,\n",
    "                          \"epochs\": 40,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model5\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.001,\n",
    "                          \"number_layers\": 150,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 200, \n",
    "                          'dropout': 0.1,\n",
    "                          'hidden_size' : 30,\n",
    "                          \"epochs\": 50,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model6\"\n",
    "                        }, \n",
    "                        {\n",
    "                          \"learning_rate\": 0.1,\n",
    "                          \"number_layers\": 10,\n",
    "                          \"embedding_dim\": 100,\n",
    "                          'batch_size' : 150, \n",
    "                          'dropout': 0,\n",
    "                          'hidden_size' : 10,\n",
    "                          \"epochs\": 20,\n",
    "                          \"device\": device,\n",
    "                          \"model_name\": \"model7\"\n",
    "                        }, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyClassifier2B(\n",
      "  (lstm): LSTM(7, 10, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (linear): Linear(in_features=20, out_features=6, bias=True)\n",
      ")\n",
      "Training... \n",
      "\n",
      "Total loss in epoch 1 is 16.083955764770508.\n",
      "Total loss in epoch 2 is 3.320253849029541.\n",
      "Total loss in epoch 3 is 3.1240499019622803.\n",
      "Total loss in epoch 4 is 3.053210496902466.\n",
      "Total loss in epoch 5 is 3.0096893310546875.\n",
      "Total loss in epoch 6 is 3.028453826904297.\n",
      "Total loss in epoch 7 is 2.958759307861328.\n",
      "Total loss in epoch 8 is 2.9714717864990234.\n",
      "Total loss in epoch 9 is 2.918546199798584.\n",
      "Total loss in epoch 10 is 2.9047181606292725.\n",
      "Total loss in epoch 11 is 2.8734774589538574.\n",
      "Total loss in epoch 12 is 2.8757128715515137.\n",
      "Total loss in epoch 13 is 2.8402743339538574.\n",
      "Total loss in epoch 14 is 2.854701042175293.\n",
      "Total loss in epoch 15 is 2.8289546966552734.\n",
      "Training Finished\n",
      "\n",
      "Evaluating... \n",
      "\n",
      "5\n",
      "5\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5]\n",
      "model: model1 Accuracy: 0.9753645047762695 Precision: 0.9538652508255003 Recall: 0.9753645047762695 F1_score: 0.9868448916320564\n",
      "MyClassifier2B(\n",
      "  (lstm): LSTM(7, 15, num_layers=10, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (linear): Linear(in_features=30, out_features=6, bias=True)\n",
      ")\n",
      "Training... \n",
      "\n",
      "Total loss in epoch 1 is 8.315218925476074.\n",
      "Total loss in epoch 2 is 4.8458571434021.\n",
      "Total loss in epoch 3 is 4.788125038146973.\n",
      "Total loss in epoch 4 is 4.58158540725708.\n",
      "Total loss in epoch 5 is 4.2291975021362305.\n",
      "Total loss in epoch 6 is 3.541271448135376.\n",
      "Total loss in epoch 7 is 2.2919347286224365.\n",
      "Total loss in epoch 8 is 1.4097682237625122.\n",
      "Total loss in epoch 9 is 1.0537097454071045.\n",
      "Total loss in epoch 10 is 0.9233865737915039.\n",
      "Total loss in epoch 11 is 0.8627159595489502.\n",
      "Total loss in epoch 12 is 0.836829662322998.\n",
      "Total loss in epoch 13 is 0.815566897392273.\n",
      "Total loss in epoch 14 is 0.8043717741966248.\n",
      "Total loss in epoch 15 is 0.7996442914009094.\n",
      "Total loss in epoch 16 is 0.7899706363677979.\n",
      "Total loss in epoch 17 is 0.7861390709877014.\n",
      "Total loss in epoch 18 is 0.7826632857322693.\n",
      "Total loss in epoch 19 is 0.7799888253211975.\n",
      "Total loss in epoch 20 is 0.7773294448852539.\n",
      "Training Finished\n",
      "\n",
      "Evaluating... \n",
      "\n",
      "5\n",
      "5\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "model: model2 Accuracy: 0.9753142282554047 Precision: 0.9538093903144835 Recall: 0.9753142282554047 F1_score: 0.9867945016285418\n",
      "MyClassifier2B(\n",
      "  (lstm): LSTM(7, 3, num_layers=15, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (linear): Linear(in_features=6, out_features=6, bias=True)\n",
      ")\n",
      "Training... \n",
      "\n",
      "Total loss in epoch 1 is 11.764100074768066.\n",
      "Total loss in epoch 2 is 9.651646614074707.\n",
      "Total loss in epoch 3 is 8.619210243225098.\n",
      "Total loss in epoch 4 is 6.145220756530762.\n",
      "Total loss in epoch 5 is 5.520096778869629.\n",
      "Total loss in epoch 6 is 5.067105770111084.\n",
      "Total loss in epoch 7 is 4.9550371170043945.\n",
      "Total loss in epoch 8 is 4.833278656005859.\n",
      "Total loss in epoch 9 is 4.873660087585449.\n",
      "Total loss in epoch 10 is 4.673556327819824.\n",
      "Total loss in epoch 11 is 4.628714561462402.\n",
      "Total loss in epoch 12 is 4.674541473388672.\n",
      "Total loss in epoch 13 is 4.696090221405029.\n",
      "Total loss in epoch 14 is 4.811949729919434.\n",
      "Total loss in epoch 15 is 4.820717811584473.\n"
     ]
    }
   ],
   "source": [
    "## hyperparamater tuning\n",
    "# train you model with your set of hyperparamaters\n",
    "for hp in set_hyperparamaters:\n",
    "    trainer.train(train_X, train_y, val_X, val_y, MyClassifier2B, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parallel_coordinates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e8f2a455a738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a parallel coordination plot over hyperparamaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# add path to models and change metric to what ever metric you have chosen to use/want to use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparallel_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parallel_coordinates' is not defined"
     ]
    }
   ],
   "source": [
    "# create a parallel coordination plot over hyperparamaters\n",
    "# add path to models and change metric to what ever metric you have chosen to use/want to use\n",
    "parallel_coordinates(model_dump, metric=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the Parallel Coordination plot define 3 new hyperparamaters set thay you \n",
    "# think are worth testing\n",
    "set_hyperparamaters_2 = [\n",
    "                        # Example:\n",
    "                        # {\n",
    "                        #   \"learning_rate\": 0.1,\n",
    "                        #   \"number_layers\": 3,\n",
    "                        #   \"optimizer\": \"adam\"\n",
    "                        # }\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models for the new hyperparamaters\n",
    "for hp in set_hyperparamaters_2:\n",
    "    trainer.train(train_X, train_y, val_X, val_y, YOUR_MODEL_CLASS, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parallel coordination plot over hyperparamaters again\n",
    "parallel_coordinates(model_dump, metric=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the best model base on the last parallel coordination plot\n",
    "best_model_path = \"PATH TO THE BEST MODEL\"\n",
    "scores = trainer.test(test_X, test_y, YOUR_MODEL_CLASS, best_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
